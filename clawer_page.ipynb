{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import urllib\n",
    "from selenium import webdriver\n",
    "from lxml import etree\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camouflage\n",
    "ua = UserAgent()\n",
    "header = {'User-Agent':ua.random}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ip():\n",
    "    ip_list = []\n",
    "    #Route\n",
    "    url = 'https://www.xicidaili.com/nt/' #ip is time-effective, only crawling the first page\n",
    "    #request\n",
    "    response = requests.get(url=url,headers=header)\n",
    "    #Set encoding\n",
    "    response.encoding = response.apparent_encoding\n",
    "    response = response.text\n",
    "\n",
    "    response = etree.HTML(response)\n",
    "\n",
    "    tr_list = response.xpath('//tr[@class=\"odd\"]')\n",
    "    for i in tr_list:\n",
    "        #ip\n",
    "        ip = i.xpath('./td[2]/text()')[0]\n",
    "        #Port number\n",
    "        port = i.xpath('./td[3]/text()')[0]\n",
    "        #Agreement\n",
    "        agreement = i.xpath('./td[6]/text()')[0]\n",
    "        agreement = agreement.lower()\n",
    "        #Complete assembly path\n",
    "        ip = agreement + '://' + ip + ':' + port\n",
    "        ip_list.append(ip)\n",
    "    return ip_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://114.55.95.112:8999\n"
     ]
    }
   ],
   "source": [
    "ip_list = get_ip()\n",
    "print(ip_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check whether the anti-clawer solution has worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapy:  https://cosmily.com/micellar-and-cleansing-water\n"
     ]
    }
   ],
   "source": [
    "# define the url to scrapy\n",
    "url_main='https://cosmily.com/micellar-and-cleansing-water'\n",
    "\n",
    "for ip in ip_list:\n",
    "    print(\"scrapy: \", url_main)\n",
    "    # set the proxy\n",
    "    proxy = urllib.request.ProxyHandler({'https':ip})\n",
    "    # create an opener\n",
    "    # opener = urllib.request.build_opener(proxy, urllib.request.HTTPHandler)\n",
    "    opener = urllib.request.build_opener(proxy)\n",
    "    # set opener as global variable\n",
    "    urllib.request.install_opener(opener)\n",
    "    try:\n",
    "        response = requests.get(url_main, headers=header)\n",
    "        break\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "\n",
    "html = BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=r'D:\\Part-Time\\geckodriver-v0.26.0-win64\\geckodriver.exe')\n",
    "driver.get(url_main)\n",
    "\n",
    "# define the three json parts to save the information\n",
    "others_json = {}\n",
    "harmfulness_json = {}\n",
    "beneficial_json = {}\n",
    "\n",
    "# report_json = {}\n",
    "# component_json = {}\n",
    "# component_ratio_json = {}\n",
    "# component_json = {}\n",
    "\n",
    "# define the list to save all product\n",
    "products_inf=[]\n",
    "harmfulness_title = []\n",
    "harmfulness_ingredient_icon = []\n",
    "harmfulness_undertitle = []\n",
    "harmfulness_style = []\n",
    "beneficial_ingredient_icon = [] \n",
    "beneficial_title = []\n",
    "beneficial_undertitle = [] \n",
    "beneficial_style = []\n",
    "\n",
    "# define the list to save products urls\n",
    "search = []\n",
    "# find the hrefs for products in main page\n",
    "products_list = driver.find_elements_by_xpath(\"//a[starts-with(@class, 'mb-2') and contains(@class, 'product-filter-title')]\")\n",
    "# save all the products hrefs at one page\n",
    "for product in products_list:\n",
    "    search.append(product.get_attribute(\"href\"))\n",
    "# build a blank json file\n",
    "with open('products.txt', 'w') as outfile:\n",
    "    json.dump([], outfile)\n",
    "\n",
    "# go through the products hrefs to grasp the information   \n",
    "for url in search:\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        # grap title data\n",
    "        category_names = driver.find_elements_by_xpath(\"//span[@itemprop='name']\")\n",
    "        others_json['category_first_name'] = category_names[2].text\n",
    "        others_json['category_second_name'] = category_names[3].text\n",
    "        # print(category_names[3].text)\n",
    "        others_json['category_third_name'] = category_names[-1].text\n",
    "\n",
    "        # grap product_images\n",
    "        images_parent = driver.find_element_by_xpath(\"//div[@class='product-image']\")\n",
    "        others_json['product_images'] = images_parent.find_element_by_xpath(\".//a/img\").get_attribute(\"src\")    \n",
    "\n",
    "        # grap product_name\n",
    "        product_name = driver.find_element_by_xpath(\"//h1[starts-with(@class, 'mt-0') and contains(@class, 'mb-2')]\").text\n",
    "        others_json['product_name'] = product_name\n",
    "        \n",
    "        try:\n",
    "            # grap country_name \n",
    "            others_json['country_name'] = driver.find_element_by_xpath(\"//img[@class='ml-1']\").get_attribute(\"data-original-title\")\n",
    "\n",
    "            # grap brand_name\n",
    "            brand_name = driver.find_element_by_xpath(\"//div[@class='mb-2']\")\n",
    "            others_json['brand_name'] = brand_name.find_element_by_xpath(\".//a\").text\n",
    "        except:\n",
    "            others_json['country_name'] = []\n",
    "            others_json['brand_name'] = []\n",
    "            pass\n",
    "        # grap description\n",
    "        others_json['description'] = driver.find_element_by_xpath(\"//p[starts-with(@class,'mb-4') and contains(@class, 'px-3')]\").text   \n",
    "\n",
    "\n",
    "        # grap harmfulness_json\n",
    "        two_categories = driver.find_elements_by_xpath(\"//div[@class = 'px-3']\") \n",
    "\n",
    "        harmfulnesses = two_categories[0].find_elements_by_xpath(\".//div[starts-with(@class, 'col-') and contains(@class, 'ingredient-analysis')]\")\n",
    "        \n",
    "        for i in range(len(harmfulnesses)):\n",
    "            harmfulness_ingredient_icon.append(harmfulnesses[i].find_element_by_xpath(\".//div/div/div/img[@class='ingredient-icon']\").get_attribute(\"src\"))\n",
    "            harmfulness_title.append(harmfulnesses[i].find_element_by_xpath(\".//div/div/div/img[@class='ingredient-icon']\").get_attribute(\"alt\")) \n",
    "            harmfulness_undertitle.append(harmfulnesses[i].find_element_by_xpath(\".//div/div/div/div/span[@class='undertitle']\").text)\n",
    "            harmfulness_style.append(harmfulnesses[i].find_element_by_xpath(\".//div/div[starts-with(@class, 'col-2')]/b\").text)\n",
    "            # harmfulness_json[\"down\"]\n",
    "        harmfulness_json['ingredient-icon'] = harmfulness_title\n",
    "        harmfulness_json['title'] = harmfulness_title\n",
    "        harmfulness_json['undertitle'] = harmfulness_undertitle\n",
    "        harmfulness_json['style'] = harmfulness_style\n",
    "        # grap beneficial_json\n",
    "        beneficial = two_categories[1].find_elements_by_xpath(\".//div[starts-with(@class, 'col-') and contains(@class, 'ingredient-analysis')]\")\n",
    "        \n",
    "        for i in range(len(beneficial)):\n",
    "            beneficial_ingredient_icon.append(beneficial[i].find_element_by_xpath(\".//div/div/div/img[@class='ingredient-icon']\").get_attribute(\"src\"))\n",
    "            beneficial_title.append(beneficial[i].find_element_by_xpath(\".//div/div/div/img[@class='ingredient-icon']\").get_attribute(\"alt\")) \n",
    "            beneficial_undertitle.append(beneficial[i].find_element_by_xpath(\".//div/div/div/div/span[@class='undertitle']\").text)\n",
    "            beneficial_style.append(beneficial[i].find_element_by_xpath(\".//div/div[starts-with(@class, 'col-2')]/b\").text)\n",
    "            # harmfulness_json[\"down\"]\n",
    "        beneficial_json[\"ingredient-icon\"] = beneficial_ingredient_icon\n",
    "        beneficial_json[\"title\"] = beneficial_title\n",
    "        beneficial_json[\"undertitle\"] = beneficial_undertitle\n",
    "        beneficial_json[\"style\"] = beneficial_style\n",
    "        # save all the information for one product\n",
    "        # print(beneficial_json)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "        \n",
    "    products_inf.append(others_json)\n",
    "    products_inf.append(harmfulness_json)\n",
    "    products_inf.append(beneficial_json)\n",
    "# save the result to products.txt\n",
    "with open('products.txt', mode='w', encoding='utf-8') as feedsjson:\n",
    "    json.dump(products_inf, feedsjson)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bitdbd76ed984a5488496eb976b9e8b3b8e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
