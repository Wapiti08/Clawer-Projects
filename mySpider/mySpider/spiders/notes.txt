scrapy.cfg: the basic settings--->functions
									Concurrent setting
									pipelines ducuments
									distributed 
									cleaning
items.py: the model of items--->scrapy.Field() stores the string
											---->use it like dict
											
pipelines.py
	the function process_item is stable
	
settings.py:
	1.concurrent requests:16 default
	2.Download_delay
	3.cookies_enabled: whether will be traced
	4.default_request_headers: if it is set, then there is no need to be set in spider again
	5.spider_middlewares: priority(if there is more than one)
	*6.Item_Pipelines:priority and can set yourself pipelines
	#7.can set the redis_host and others(in distributed system)

edit the spider:
	in the directory of spiders:
	use the command:scrapy genspider name(muspider) scope(incast.cn)

In the beginning, we'd better make some notes,like 
	
//表示任意目录
teacher_list=response.xpath('//div[@class="li_txt"]'):
for each in teacher_list:
	#name
	each.xpath('./h3/text()')
	#title
	each.xpath('./h4/text()')
	#info
	each.xpath('./p/text()')

then begins the program
1.set the items.py:
	load the libirary
	define the items: inhert the scrapy.Item
2.begin the spider:
	1.name
	2.allowd_domains
	3.start_urls (can be more)
	*4.parse
		response.body----special
execute:scrapy crawl name